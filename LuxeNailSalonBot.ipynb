{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andege19/LuxeChatBot/blob/main/LuxeNailSalonBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Luxe Nails Kenya Chatbot\n",
        "###The chatbot is designed to handle user queries related to a nail salon, providing responses in either English or Swahili based on the input language. The process involves loading and preprocessing a dataset, training a neural network model, and implementing a user interface for interaction. We are to:\n",
        "\n",
        "<ul>\n",
        "<li>Train a rule-based chatbot using this dataset.\n",
        "<li>Use ML/NLP (like TF-IDF + cosine similarity) to make it smarter.\n",
        "<li>Add Swahili / Sheng phrasing for the local language flavor.\n",
        "<ul>\n"
      ],
      "metadata": {
        "id": "VpCqo-euhsbb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Building Process\n",
        "###Dataset Loading and Preprocessing\n",
        "<ul>\n",
        "<li>Loading: The dataset is loaded from a CSV file, containing intents, patterns, and responses.\n",
        "<li>Language Mapping: A function (detect_lang) is used to map responses to either English or Swahili based on specific keywords or default settings.\n",
        "<li>Preprocessing: Text preprocessing includes tokenization, lowercasing, stopword removal, stemming, and lemmatization to prepare the data for training.\n",
        "<ul>\n",
        "\n",
        "###Training Data Creation\n",
        "<ul>\n",
        "<li>Bag-of-Words Model: The preprocessed patterns are converted into a bag-of-words representation, creating input features for the neural network.\n",
        "<li>Labels: Intents are encoded as labels for supervised learning.\n",
        "<ul>\n",
        "\n",
        "###Neural Network Model\n",
        "<ul>\n",
        "<li>Architecture: A simple neural network with one hidden layer is defined.\n",
        "<li>Training: The model is trained using backpropagation to learn the relationship between input patterns and intents.\n",
        "\n",
        "###Response Handling\n",
        "<ul>\n",
        "<li>Intent Prediction: The trained model predicts the intent of user input.\n",
        "<li>Language Detection: The input language is detected to select the appropriate response language.\n",
        "<li>Response Selection: A response is chosen based on the predicted intent and detected language.\n",
        "<ul>\n",
        "\n",
        "####User Interface\n",
        "<ul>\n",
        "<li>Gradio Interface: A user-friendly interface is created using Gradio, allowing users to interact with the chatbot via text input.\n",
        "<>ul"
      ],
      "metadata": {
        "id": "gHbPaI5Iir93"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Testing Process\n",
        "###Unit Testing\n",
        "Language Detection: Test the detect_input_lang function with various inputs to ensure it correctly identifies English and Swahili.\n",
        "Preprocessing: Verify that the preprocess function correctly tokenizes, removes stopwords, and applies stemming and lemmatization.\n",
        "Intent Prediction: Test the neural network's ability to predict the correct intent for a variety of input patterns.\n",
        "\n",
        "###Integration Testing\n",
        "End-to-End Testing: Simulate user interactions through the Gradio interface to ensure the entire system works cohesively.\n",
        "Response Accuracy: Check if the chatbot provides accurate and contextually appropriate responses based on the input language and intent.\n",
        "\n",
        "###Performance Testing\n",
        "Response Time: Measure the time it takes for the chatbot to process an input and generate a response to ensure it meets performance expectations.\n",
        "Scalability: Test the chatbot with a large number of queries to evaluate its performance under load.\n",
        "\n",
        "###User Acceptance Testing\n",
        "Scenario Testing: Test the chatbot with various scenarios, including edge cases and ambiguous inputs, to see how it handles unexpected situations.\n",
        "\n",
        "###Challenges in Testing\n",
        "Language Ambiguity: Handling inputs that could be interpreted in multiple languages or have ambiguous meanings.\n",
        "Intent Overlap: Dealing with inputs that could belong to multiple intents, requiring the chatbot to make a nuanced decision.\n",
        "Data Variability: Ensuring the test data covers a wide range of possible inputs to thoroughly evaluate the chatbot's performance.\n",
        "\n",
        "###Areas for Improvement in Testing\n",
        "Automated Testing: Implement automated tests for language detection, preprocessing, and intent prediction to catch regressions early.\n",
        "Expanded Test Data: Create a comprehensive test dataset that includes diverse examples, edge cases, and mixed-language inputs.\n",
        "Continuous Integration: Integrate testing into the development pipeline to automatically run tests with each code change.\n",
        "User Feedback Loop: Establish a feedback loop with users to continuously gather insights and improve the chatbot based on real-world usage.\n",
        "Error Logging: Implement detailed error logging to capture and analyze failures during testing and live usage."
      ],
      "metadata": {
        "id": "w6g24Y5DlFbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Challenges\n",
        "<ul>\n",
        "<li>Language Detection: Accurately detecting the input language, especially when inputs are short or contain mixed languages, can be challenging.\n",
        "<li>Dataset Quality: Ensuring the dataset is comprehensive and accurately labeled is crucial for model performance.\n",
        "<li>Model Complexity: Balancing model complexity with performance to ensure real-time response capability.\n",
        "<li>Handling Ambiguity: Dealing with ambiguous user inputs that could belong to multiple intents.\n",
        "<ul>"
      ],
      "metadata": {
        "id": "htxh0zCkj-G0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Areas for Improvement\n",
        "<ul>\n",
        "<li>Enhanced Language Detection: Implement more sophisticated language detection algorithms to improve accuracy, especially for mixed-language inputs.\n",
        "<li>Dataset Expansion: Expand the dataset to include more diverse examples and languages if needed.\n",
        "<li>Model Optimization: Experiment with different neural network architectures and hyperparameters to improve accuracy and response time.\n",
        "<li>User Feedback Integration: Incorporate user feedback to continuously improve the chatbot's performance and response quality.\n",
        "<li>Error Handling: Improve error handling for cases where the input language is not clearly identifiable or the intent is ambiguous.\n",
        "<li>Multilingual Support: Consider extending support to additional languages if the user base expands.\n",
        "<ul>"
      ],
      "metadata": {
        "id": "lcLfWHO3kdh6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Installing Dependencies"
      ],
      "metadata": {
        "id": "1-fxsGJkCZPC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iw-PFXWNBdPS",
        "outputId": "34953bcb-9c78-4aae-cd26-768af05e6981"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.24.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.8.0 (from gradio)\n",
            "  Downloading gradio_client-1.8.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.2)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.24.0-py3-none-any.whl (46.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.9/46.9 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.8.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.2/322.2 kB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m79.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993223 sha256=cf7c2d846896612a4955ec2d0c037dde583337f5122bc3adc81e4faac04636bc\n",
            "  Stored in directory: /root/.cache/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
            "Successfully built langdetect\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, langdetect, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.24.0 gradio-client-1.8.0 groovy-0.1.2 langdetect-1.0.9 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.4 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install pandas numpy nltk langdetect gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing Libraries"
      ],
      "metadata": {
        "id": "srrGCd2WCep_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import dependencies\n",
        "import pandas as pd #tool for working with structured data like CSV files or Excel spreadsheets.\n",
        "import numpy as np  # adds support for large, multi-dimensional arrays and mathematical functions.\n",
        "import nltk # library for working with human language data (text).\n",
        "import random # built-in module to generate random numbers or make random choices — useful for sampling, shuffling\n",
        "import gradio as gr\n",
        "from nltk.stem import LancasterStemmer #A stemmer-chops off word endings to reduce words to their root form (more aggressive).\n",
        "from langdetect import detect, DetectorFactory  # detect: Function to auto-detect the language of a given text string.\n",
        "#DetectorFactory: Lets you configure how detection works\n",
        "from langdetect.lang_detect_exception import LangDetectException #LangDetectException: Used to catch errors when language detection fails (like for empty or gibberish text).\n",
        "\n",
        "# Download NLTK tokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Set seed for consistent language detection\n",
        "DetectorFactory.seed = 0\n",
        "stemmer = LancasterStemmer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jvN2askDBcR",
        "outputId": "bb93686c-48b9-4be4-ce5e-4b0ef9d3f470"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading Dataset"
      ],
      "metadata": {
        "id": "c1-mbwTJCiEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_dataset(file_path):\n",
        "    try:\n",
        "        df = pd.read_csv(file_path)\n",
        "        data = {\"intents\": []}  #initializes an empty dictionary data with a key \"intents\" that has an empty list as its value.\n",
        "    #This dictionary will hold the final structure of the dataset, where each intent will be organized into different categories.\n",
        "\n",
        "\n",
        "        for intent_group in df.groupby('Intent'): #This line begins a loop that groups the data in the DataFrame df by the 'Intent' column.\n",
        "    #The variable intent will hold the name of each intent group (e.g., \"greeting\", \"service inquiry\"), and group will hold the subset of rows corresponding to that particular intent.\n",
        "\n",
        "            intent = intent_group[0]\n",
        "            group_data = intent_group[1]\n",
        "            intent_entry = {\n",
        "                \"tag\": intent,\n",
        "                \"patterns\": [],\n",
        "                \"responses\": {\"en\": [], \"sw\": []}\n",
        "            }\n",
        "\n",
        "            for _, row in group_data.iterrows():#Loops through each row of the grouped data for that intent.\n",
        "                intent_entry[\"patterns\"].append(row['Pattern'])#Adds the user input pattern from the 'Pattern' column to the patterns list.\n",
        "                response_text = row['Response'] #Grabs the chatbot's response from the 'Response' column.\n",
        "                try:\n",
        "                    lang = detect(response_text)#Uses the langdetect library's detect() function to guess the language of the response.\n",
        "                    if lang not in ['en', 'sw']:\n",
        "                        lang = 'en' if any(c in 'abcdefghijklmnopqrstuvwxyz' for c in response_text.lower()) else 'sw' #If the language is not English (en) or Swahili (sw), it does a manual check:\n",
        "                        #If it finds English-like letters, it assumes it's English.Otherwise, it assumes Swahili.\n",
        "\n",
        "                except LangDetectException:#If language detection fails or errors, default to English.\n",
        "                    lang = 'en'\n",
        "                intent_entry[\"responses\"][lang].append(response_text)#Appends the response text to the appropriate language list (\"en\" or \"sw\").\n",
        "\n",
        "            data[\"intents\"].append(intent_entry)#After processing all rows for an intent, add the intent_entry to the data[\"intents\"] list.\n",
        "        return data #Return the final data dictionary, structured for chatbot training.\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        return None #If anything goes wrong during file reading or processing, print the error and return None.\n",
        "\n",
        "dataset = load_dataset('nail_salon_chatbot_100questions_dataset.csv')\n"
      ],
      "metadata": {
        "id": "QMnGz2a3DJlE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing"
      ],
      "metadata": {
        "id": "xmfdbqgDCkMG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_and_detect_language(sentence): #This defines a function called preprocess_and_detect_language that takes one argument: sentence (a string — user input).\n",
        "    try:\n",
        "        lang = detect(sentence)#Uses langdetect.detect() to automatically identify the language of the input sentence.(For example, \"Habari yako?\" would likely return 'sw' (Swahili), while \"Hello, how are you?\" would return 'en')\n",
        "\n",
        "    except LangDetectException:\n",
        "        lang = 'en'#If language detection fails or throws an error (e.g., input is gibberish or empty), the code defaults to 'en' (English) as a fallback.\n",
        "\n",
        "    words = nltk.word_tokenize(sentence) #Uses NLTK to split the sentence into individual words (called tokens).Example: \"Hi there!\" → ['Hi', 'there', '!']\n",
        "\n",
        "    stemmed_words = [stemmer.stem(word.lower()) for word in words] #Converts all words to lowercase and applies stemming to reduce words to their root form:\"running\" → \"run\", \"easily\" → \"eas\". It assumes stemmer is defined elsewhere (likely nltk.PorterStemmer() or SnowballStemmer()).\n",
        "    return stemmed_words, lang #Returns two things: A list of stemmed, lowercased tokens (for model input or analysis)\n",
        "    #The detected language code ('en' or 'sw', etc.)"
      ],
      "metadata": {
        "id": "NzMQpuBYD9Ru"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creating Training Data"
      ],
      "metadata": {
        "id": "Q6otjq-BCnpd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install pandas numpy nltk langdetect gradio\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def create_training_data(data): #Takes data (like the dataset we prepared earlier) and converts it into machine learning-friendly format.\n",
        "    words = [] #(words → all the words in patterns, labels → all unique intent tags (e.g., greeting, booking, etc.), docs → list of tuples: (pattern words, tag))\n",
        "    labels = []\n",
        "    docs = []\n",
        "\n",
        "    for intent in data['intents']:#(Loops over each intent.Tokenizes each pattern (user phrase) into words.Adds these words to the global words list.Saves (tokenized pattern, tag) into docs.)\n",
        "        for pattern in intent['patterns']:\n",
        "            wrds = nltk.word_tokenize(pattern)\n",
        "            words.extend(wrds)\n",
        "            docs.append((wrds, intent['tag']))\n",
        "\n",
        "        if intent['tag'] not in labels: #Adds the tag (intent label) to the labels list if it's not already there.\n",
        "            labels.append(intent['tag'])\n",
        "\n",
        "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]#Stems and lowercases all words, and removes duplicates using set().\n",
        "    #[\"running\", \"run\", \"ran\"] might all become \"run\"words is now a sorted list of all unique root words across all patterns.\n",
        "    words = sorted(list(set(words)))\n",
        "\n",
        "    training = []#training will hold \"bag of words\" vectors for input\n",
        "    output = [] #output will hold one-hot vectors representing the intent tag\n",
        "\n",
        "    for doc in docs:\n",
        "      #(Creates a bag of words vector (binary vector).\n",
        "      #For every word in the full words list, check if it exists in this doc's pattern.The result: a list of 1s and 0s indicating presence/absence of each word.)\n",
        "        bag = [1 if stemmer.stem(w.lower()) in [stemmer.stem(word.lower()) for word in doc[0]] else 0 for w in words]\n",
        "\n",
        "        output_row = [0] * len(labels)#(Creates a one-hot encoded vector for the output intent:If the intent is the second label, output would be: [0, 1, 0, 0, ...])\n",
        "        output_row[labels.index(doc[1])] = 1\n",
        "\n",
        "        training.append(bag)#Appends the bag and label vector to the training set.\n",
        "        output.append(output_row)\n",
        "\n",
        "    return np.array(training), np.array(output), words, labels #(training as X_train – input features, output as y_train – labels, words – the vocabulary used, labels – the intent classes)\n",
        "\n",
        "X_train, y_train, all_words, tags = create_training_data(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JirfXGg8EDnf",
        "outputId": "95848f6d-8cac-4b2e-a33b-b490fc421922"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.11/dist-packages (1.0.9)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from langdetect) (1.17.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.4)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.1)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building Neural Network"
      ],
      "metadata": {
        "id": "FhEOKlEICq_M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Neural Network Training ---\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def train_model(X, y, hidden_neurons=10, alpha=0.1, epochs=10000):\n",
        "#Trains a basic 2-layer neural network (input → hidden → output) Parameters:\n",
        "#X: Input data (bag of words)\n",
        "#y: Output labels (one-hot vectors)\n",
        "#hidden_neurons: Number of neurons in the hidden layer\n",
        "#alpha: Learning rate (controls weight update step size)\n",
        "#epochs: Number of training loops\n",
        "\n",
        "    np.random.seed(1) #Sets a seed so that weight initialization is consistent across runs.\n",
        "    input_neurons = X.shape[1] #Number of input and output neurons depends on input feature size and output classes.\n",
        "    output_neurons = y.shape[1]\n",
        "\n",
        "    synapse_0 = 2 * np.random.random((input_neurons, hidden_neurons)) - 1 #(synapse_0: Weights between input → hidden layer synapse_1: Weights between hidden → output layer. Random values between -1 and 1)\n",
        "    synapse_1 = 2 * np.random.random((hidden_neurons, output_neurons)) - 1\n",
        "\n",
        "    for epoch in range(epochs): #Run the learning steps for a fixed number of epochs.\n",
        "        layer_0 = X # input features\n",
        "        layer_1 = sigmoid(np.dot(layer_0, synapse_0)) # hidden layer activations\n",
        "        layer_2 = sigmoid(np.dot(layer_1, synapse_1)) #output predictions (probabilities of each intent)\n",
        "\n",
        "        layer_2_error = y - layer_2 #Difference between true labels and predictions.\n",
        "        layer_2_delta = layer_2_error * (layer_2 * (1 - layer_2)) #Derivative of sigmoid (for gradient) = sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "        layer_1_error = layer_2_delta.dot(synapse_1.T) #Same backpropagation logic applied to the hidden layer\n",
        "        layer_1_delta = layer_1_error * (layer_1 * (1 - layer_1))\n",
        "\n",
        "        synapse_1 += alpha * layer_1.T.dot(layer_2_delta) #(Weights are adjusted using gradient descent:Multiply the error gradient with learning rate and dot product with the previous layer’s output.This makes the network slightly more accurate each time.)\n",
        "        synapse_0 += alpha * layer_0.T.dot(layer_1_delta)\n",
        "\n",
        "    return synapse_0, synapse_1 #These trained weights (synapse_0, synapse_1) will be used for making predictions with the chatbot.\n"
      ],
      "metadata": {
        "id": "k-lWVSI-FJbr"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine Tuning the Model"
      ],
      "metadata": {
        "id": "vz8Z2E_vJdNO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fine_tune_model(new_data, synapse_0, synapse_1, words, tags, epochs=1000, alpha=0.1): # new_data - A list of new patterns and their correct intent tags (like extra training examples)\n",
        "    new_training = []#stores the new training input and output data.\n",
        "    new_output = []\n",
        "\n",
        "    for entry in new_data: #Tokenizes and stems each new input phrase (just like in training).\n",
        "        pattern = entry['pattern']\n",
        "        tag = entry['tag']\n",
        "        wrds = nltk.word_tokenize(pattern)\n",
        "        pattern_words = [stemmer.stem(w.lower()) for w in wrds]\n",
        "\n",
        "        bag = [1 if w in pattern_words else 0 for w in words]#Builds a bag-of-words vector based on whether each known word is in the input.\n",
        "        new_training.append(bag)\n",
        "\n",
        "#Creates a one-hot vector for the tag — only if it exists in the original tag list.If it’s a brand new tag not in the model? Skip it (nice and safe).\n",
        "        output_row = [0] * len(tags)\n",
        "        if tag in tags:\n",
        "            output_row[tags.index(tag)] = 1\n",
        "        else:\n",
        "            print(f\"[!] Tag '{tag}' not in existing tags. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        new_output.append(output_row) #Adds the output vector to the training list.\n",
        "\n",
        "#Converts lists into arrays for training math.\n",
        "    X_new = np.array(new_training)\n",
        "    y_new = np.array(new_output)\n",
        "\n",
        "#The fine tuning loop is the exact same backpropagation logic as the main training function. The key difference is: you're only updating the model with new examples.\n",
        "    for _ in range(epochs):\n",
        "        layer_0 = X_new\n",
        "        layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n",
        "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
        "\n",
        "        layer_2_error = y_new - layer_2\n",
        "        layer_2_delta = layer_2_error * (layer_2 * (1 - layer_2))\n",
        "\n",
        "        layer_1_error = layer_2_delta.dot(synapse_1.T)\n",
        "        layer_1_delta = layer_1_error * (layer_1 * (1 - layer_1))\n",
        "\n",
        "        synapse_1 += alpha * layer_1.T.dot(layer_2_delta)\n",
        "        synapse_0 += alpha * layer_0.T.dot(layer_1_delta)\n",
        "\n",
        "#Returns the fine-tuned weights, ready to use for predictions.\n",
        "    return synapse_0, synapse_1\n"
      ],
      "metadata": {
        "id": "Exy4YjGII657"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test for Accuracy"
      ],
      "metadata": {
        "id": "O1LzqhFeCuLG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Trains the model using previously defined train_model() function.synapse_0 and synapse_1 now hold the trained weights for inference (prediction).\n",
        "synapse_0, synapse_1 = train_model(X_train, y_train)\n",
        "\n",
        "# --- Response Generation ---\n",
        "#function\n",
        "def get_response(intent_tag, input_lang):\n",
        "\n",
        "  #Looks through each intent in the dataset.When it finds a match with intent_tag, it proceeds to find the correct response.\n",
        "    for intent in dataset['intents']:\n",
        "        if intent['tag'] == intent_tag:\n",
        "\n",
        "  #Tries to get the response list in the user’s language (e.g., 'en' or 'sw').If no such responses exist for the language, it will default to English:\n",
        "            lang_responses = intent['responses'].get(input_lang, [])\n",
        "          #Randomly pick a response so replies aren’t repetitive:\n",
        "            if not lang_responses:\n",
        "                lang_responses = intent['responses']['en']\n",
        "            return random.choice(lang_responses)\n",
        "\n",
        "    # Fallback. Used when no matching intent is found.\n",
        "    fallback = {\n",
        "        \"en\": \"I'm sorry, I don't have information on that. Please visit our website https://luxenails.co.ke or contact Luxe Nails directly.\",\n",
        "        \"sw\": \"Samahani, sina habari kuhusu hayo. Tafadhali tembelea tovuti yetu https://luxenails.co.ke au wasiliana na Luxe Nails moja kwa moja.\",\n",
        "\n",
        "    }\n",
        "    return fallback.get(input_lang, fallback[\"en\"])\n",
        "\n",
        "#Fuction\n",
        "#Main function to get a chatbot response for a user message.\n",
        "def chatbot_response(user_input, language=\"en\"):\n",
        "    try:\n",
        "      #Tokenizes and stems input. Tries to detect the input language using langdetect.\n",
        "        processed_input, detected_lang = preprocess_and_detect_language(user_input)\n",
        "\n",
        "        #Matches the user’s input words against the full vocabulary.\n",
        "        bag = [1 if word in processed_input else 0 for word in all_words]\n",
        "\n",
        "      #Just like during training, except here we're only predicting (no backpropagation). Final output layer_2 is a vector of probabilities for each intent.\n",
        "        layer_0 = np.array(bag)\n",
        "        layer_1 = sigmoid(np.dot(layer_0, synapse_0))\n",
        "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))\n",
        "\n",
        "      #Gets the index of the highest-probability tag. Maps it back to the tag name.\n",
        "        intent_tag = tags[np.argmax(layer_2)]\n",
        "        # Fetches an appropriate response\n",
        "        response = get_response(intent_tag, language)\n",
        "        return response\n",
        "    #Prevents the chatbot from crashing if something unexpected happens.\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return \"Sorry, something went wrong.\""
      ],
      "metadata": {
        "id": "3H2idYbZFZOh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chatbot Interface"
      ],
      "metadata": {
        "id": "0XNhEtXAC02Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Gradio Interface ---\n",
        "# Language Bridge Function\n",
        "def chatbot_interface(Questions:str, language:str):\n",
        "\n",
        "  #This dictionary maps the dropdown label (\"English\", \"Swahili\") to language codes used by your chatbot (\"en\", \"sw\").\n",
        "    lang_map = {\"English\": \"en\", \"Swahili\": \"sw\",}\n",
        "\n",
        "    #Calls response function. It feeds the question and mapped language code to your chatbot_response() function.\n",
        "    return chatbot_response(Questions, lang_map[language])\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=chatbot_interface, #The function to be called when user interacts.\n",
        "    inputs=[\"text\", gr.Dropdown([\"English\", \"Swahili\",], value=\"English\")], # For user to type a question.Let’s user choose between English or Swahili.\n",
        "    outputs=\"text\", #A single text output (the chatbot’s response).\n",
        "    title=\"💅 Luxe Nails Chatbot\", #What’s displayed on the interface.\n",
        "    description=\"Ask me anything about Luxe Nails! Services, prices, appointments, and more.\"\n",
        ")\n",
        "\n",
        "iface.launch()#Launches the web UI locally on Colab\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 645
        },
        "id": "5sU9WFWVGCze",
        "outputId": "dddec41e-adc9-4eff-b930-09dd90b9647b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://0ed3ba8141c8067430.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0ed3ba8141c8067430.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    }
  ]
}
